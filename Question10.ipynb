{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Question10.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "oSekzMvn5dRY"
      },
      "outputs": [],
      "source": [
        "from keras.datasets import mnist\n",
        "import numpy as np\n",
        "from  matplotlib import pyplot as plt\n",
        "import time\n",
        "import math\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset= mnist.load_data()\n",
        "(X_train_and_validation, y_train_and_validation), (X_test, y_test) = dataset\n",
        "X_train, X_validation, y_train, y_validation = train_test_split(X_train_and_validation, y_train_and_validation, test_size=0.1, random_state=42)\n",
        "X_train = (X_train/255.0).astype(np.float32)\n",
        "X_validation = (X_validation/255.0).astype(np.float32)\n",
        "X_test = (X_test/255.0).astype(np.float32)\n",
        "\n",
        "X_train = np.array(X_train.reshape(X_train.shape[0], 784,1))         \n",
        "X_test = np.array(X_test.reshape(X_test.shape[0], 784,1))\n",
        "X_validation = np.array(X_validation.reshape(X_validation.shape[0], 784,1))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NGEpuGng5l0_",
        "outputId": "13917553-e8e5-497f-de17-fb3d659bcb84"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n",
            "11501568/11490434 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.imshow(X_train[78].reshape(28,28))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "x5RXB7Ph5rTc",
        "outputId": "065c9307-4eb7-46ac-b0d6-f26545aca970"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f0c0510d810>"
            ]
          },
          "metadata": {},
          "execution_count": 7
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "labels = {0:'0', 1: '1', 2:'2', 3:'3', 4:'4', 5:'5', 6:'6', 7:'7', 8:'8', 9:'9'}"
      ],
      "metadata": {
        "id": "C75_u1GW5ube"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Activation function\n",
        "def activation(activation_function):\n",
        "  if activation_function == 'sigmoid':\n",
        "    return sigmoid\n",
        "  if activation_function == 'tanh':\n",
        "    return tanh\n",
        "  if activation_function == 'ReLU':\n",
        "    return relu\n",
        "\n",
        "def sigmoid(x, derivative = False):\n",
        "  if derivative:\n",
        "    return sigmoid(x)*(1-sigmoid(x))\n",
        "  return 1/(1 + np.exp(-x))  \n",
        "\n",
        "def tanh(x, derivative = False):\n",
        "  if derivative:\n",
        "    return 1 - tanh(x)**2\n",
        "  return (np.exp(x)-np.exp(-x))/(np.exp(x)+np.exp(-x))\n",
        "\n",
        "def relu(x, derivative = False):\n",
        "  if derivative:\n",
        "    return (x>0)*1 \n",
        "  return x*(x>0)\n",
        "\n",
        "def softmax(x,derivative = False):\n",
        "  if derivative:\n",
        "    return softmax(x)*(1- softmax(x))\n",
        "  return np.exp(x)/np.sum(np.exp(x), axis = 0)\n",
        "\n",
        "def one_hot(y, num_output_nodes):\n",
        "  v = np.zeros((num_output_nodes, len(y)))\n",
        "  for i,j in enumerate(y):\n",
        "    v[j,i] = 1\n",
        "  return v"
      ],
      "metadata": {
        "id": "8oYn5-bl6rfF"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Weight Initialization\n",
        "def param_inint(num_inputs_nodes, hidden_layers, num_output_nodes, init_type):\n",
        "  W = []\n",
        "  B = []\n",
        "  if init_type == \"random\":\n",
        "    W.append(np.random.randn(hidden_layers[0],num_inputs_nodes)*0.1)\n",
        "    B.append(np.random.randn(hidden_layers[0], 1)*0.1)\n",
        "    for i in range(len(hidden_layers)-1):\n",
        "      W.append(np.random.randn(hidden_layers[i+1],hidden_layers[i])*0.1)\n",
        "      B.append(np.random.randn(hidden_layers[i+1], 1)*0.1)\n",
        "    W.append(np.random.randn(num_output_nodes, hidden_layers[-1])*0.1)\n",
        "    B.append(np.random.randn(num_output_nodes, 1)*0.1)\n",
        "    return W, B\n",
        "\n",
        "  if init_type == \"xavier\":\n",
        "    W.append(np.random.randn(hidden_layers[0],num_inputs_nodes)*np.sqrt(2/(hidden_layers[0] + num_inputs_nodes)))\n",
        "    B.append(np.random.randn(hidden_layers[0], 1)*0.1)\n",
        "    for i in range(len(hidden_layers)-1):\n",
        "      W.append(np.random.randn(hidden_layers[i+1],hidden_layers[i])*np.sqrt(2/(hidden_layers[i+1] + hidden_layers[i])))\n",
        "      B.append(np.random.randn(hidden_layers[i+1], 1)*0.1)\n",
        "    W.append(np.random.randn(num_output_nodes, hidden_layers[-1])*np.sqrt(2/(num_output_nodes + hidden_layers[-1])))\n",
        "    B.append(np.random.randn(num_output_nodes, 1)*0.1)\n",
        "    return W, B\n"
      ],
      "metadata": {
        "id": "8sPOVOYZ7UIq"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Feed forward and Back Prop"
      ],
      "metadata": {
        "id": "6RHNp7k87h-X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def feed_forward(x, W, B, activation_type):\n",
        "  h = []\n",
        "  a = []\n",
        "  sigma = activation(activation_type)  #activation\n",
        "  h.append(x)   #h0 = x\n",
        "  a.append(np.dot(W[0], h[0]) + B[0])\n",
        "  for i in range(len(W)-1):\n",
        "    h.append(sigma(a[-1]))\n",
        "    a.append(np.dot(W[i+1], h[-1]) + B[i+1])\n",
        "  y_hat = softmax(a[-1])\n",
        "\n",
        "  return y_hat, h, a\n",
        "\n",
        "def loss_compute(y,y_hat, loss_type, W, reg_lamda):\n",
        "  if loss_type == \"squared_error\":\n",
        "    error = np.sum((one_hot(y, 10)-y_hat)**2)/(2*one_hot(y, 10).shape[1])\n",
        "  if loss_type == \"cross_entropy\":\n",
        "    error = -1*np.sum(np.multiply(one_hot(y, 10),np.log(y_hat)))/one_hot(y, 10).shape[1]         # hardcoded classes = 10\n",
        "\n",
        "  if W:\n",
        "    r = 0\n",
        "    for i in range(len(W)):\n",
        "      r += np.sum((np.array(W, dtype = object) **2)[i])\n",
        "    error = error + reg_lamda * r\n",
        "\n",
        "  return error\n",
        "\n",
        "def back_prop(x, y, y_hat, a, h , W, B, batch_size, loss_type, activation_type):\n",
        "  gh = [0]*len(h)\n",
        "  ga = [0]*len(a)\n",
        "  gw = [0]*len(W)\n",
        "  gb = [0]*len(B)\n",
        "\n",
        "  sigma = activation(activation_type) \n",
        "\n",
        "  if loss_type == \"cross_entropy\":\n",
        "    gh[-1] = -1*(y/y_hat)\n",
        "    ga[-1] = -1*(y-y_hat)\n",
        "  if loss_type == \"squared_error\":   ##### edit this\n",
        "    gh[-1] = y_hat - y\n",
        "    ga[-1] = (y_hat - y)*softmax(a[-1])*(1-softmax(a[-1]))\n",
        "\n",
        "  for i in range(len(W)-1, -1, -1):\n",
        "    gw[i] = np.dot(ga[i], h[i].T)\n",
        "    gb[i] = np.dot(ga[i], np.ones((batch_size,1)))\n",
        "    if i > 0:\n",
        "      gh[i-1] = np.dot(W[i].T, ga[i])\n",
        "      ga[i-1]  = np.multiply(gh[i-1],sigma(a[i-1], derivative = True))\n",
        "\n",
        "  return gw, gb, gh, ga\n",
        "\n",
        "def accuracy(y_hat, y_true):\n",
        "  return np.mean(np.argmax(y_hat, axis = 0) ==y_true )*100\n",
        "\n"
      ],
      "metadata": {
        "id": "YWN3RVVw7aGC"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SGD:\n",
        "  ''' Stochastic Gradient Descent '''\n",
        "  def __init__(self, lr = 0.001, reg = 0):\n",
        "    self.lr = lr\n",
        "    self.reg = reg\n",
        "  \n",
        "  def update(self, w,b, gW, gB):\n",
        "    W = np.array(w, dtype = object)\n",
        "    B = np.array(b, dtype = object)\n",
        "\n",
        "    W = (1-self.lr*self.reg)*W - self.lr * np.array(gW, dtype = object)\n",
        "    B = (1-self.lr*self.reg)*B - self.lr * np.array(gB, dtype = object)\n",
        "\n",
        "    return W.tolist(),B.tolist()\n",
        "\n",
        "\n",
        "class Momentum:\n",
        "\n",
        "  def __init__(self, lr = 0.001, gamma = 0.9, reg = 0):\n",
        "    self.lr = lr\n",
        "    self.gamma = gamma\n",
        "    self.Wmoments = None\n",
        "    self.Bmoments = None\n",
        "    self.reg = reg\n",
        "\n",
        "\n",
        "  def update(self, w,b, gW, gB):\n",
        "    params = {'w':w, 'b':b}\n",
        "\n",
        "    if self.Wmoments == None:\n",
        "      self.Wmoments = [0] * len(params['w'])\n",
        "      for i in range(len(params['w'])):\n",
        "        self.Wmoments[i] = np.zeros_like(params['w'][i])\n",
        "\n",
        "    if self.Bmoments == None:\n",
        "      self.Bmoments = [0] * len(params['b'])\n",
        "      for i in range(len(params['b'])):\n",
        "        self.Bmoments[i] = np.zeros_like(params['b'][i])\n",
        "    \n",
        "    self.Wmoments = self.gamma * np.array(self.Wmoments, dtype = object) + self.lr * np.array(gW, dtype = object)\n",
        "    W = (1-self.lr*self.reg)*np.array(params['w'], dtype = object) - self.Wmoments\n",
        "    self.Wmoments = self.Wmoments.tolist()\n",
        "\n",
        "    self.Bmoments = self.gamma * np.array(self.Bmoments, dtype = object) + self.lr * np.array(gB, dtype = object)\n",
        "    B = (1-self.lr*self.reg)*np.array(params['b'], dtype = object) - self.Bmoments\n",
        "    self.Bmoments = self.Bmoments.tolist()\n",
        "    \n",
        "    return W.tolist(), B.tolist()\n",
        "\n",
        "\n",
        "class AdaGrad:\n",
        "  def __init__(self, lr=0.01):\n",
        "    self.lr = lr\n",
        "    self.vW = None\n",
        "    self.vB = None\n",
        "    self.reg = None\n",
        "      \n",
        "  def update(self, w,b, gW, gB):\n",
        "    params = {'w':w, 'b':b}\n",
        "\n",
        "    if self.vW == None:\n",
        "      self.vW = [0] * len(params['w'])\n",
        "      for i in range(len(params['w'])):\n",
        "          self.vW[i] = np.zeros_like(params['w'][i])\n",
        "\n",
        "    if self.vB == None:\n",
        "      self.vB = [0] * len(params['b'])\n",
        "      for i in range(len(params['b'])):\n",
        "          self.vB[i] = np.zeros_like(params['b'][i])\n",
        "\n",
        "    self.vW = np.array(self.vW, dtype = object) + np.array(gW, dtype = object) **2 \n",
        "    W = (1-self.lr*self.reg)*np.array(params['w'], dtype = object) - (self.lr/((self.vW + 1e-7)**0.5)) * np.array(gW, dtype = object)\n",
        "    self.vW = self.vW.tolist()\n",
        "\n",
        "    self.vB = np.array(self.vB, dtype = object) + np.array(gB, dtype = object) **2 \n",
        "    B = (1-self.lr*self.reg)*np.array(params['b'], dtype = object) - (self.lr/((self.vB + 1e-7)**0.5)) * np.array(gB, dtype = object)\n",
        "    self.vB = self.vB.tolist()\n",
        "\n",
        "    return W.tolist(), B.tolist()\n",
        "\n",
        "class RMSprop:\n",
        "  def __init__(self, lr=0.01, beta = 0.99):\n",
        "    \n",
        "    self.lr = lr\n",
        "    self.vW = None\n",
        "    self.vB = None\n",
        "    self.beta = beta\n",
        "    self.reg = None\n",
        "\n",
        "  def update(self, w,b, gW, gB):\n",
        "    params = {'w':w, 'b':b}\n",
        "    if self.vW == None:\n",
        "      self.vW = [0] * len(params['w'])\n",
        "      for i in range(len(params['w'])):\n",
        "        self.vW[i] = np.zeros_like(params['w'][i])\n",
        "\n",
        "    if self.vB == None:\n",
        "      self.vB = [0] * len(params['b'])\n",
        "      for i in range(len(params['b'])):\n",
        "        self.vB[i] = np.zeros_like(params['b'][i])\n",
        "\n",
        "    self.vW = self.beta*np.array(self.vW, dtype = object) + (1-self.beta)*(np.array(gW, dtype = object) **2) \n",
        "    W = (1-self.lr*self.reg)*np.array(params['w'], dtype = object) - (self.lr/((self.vW + 1e-7)**0.5)) * np.array(gW, dtype = object)\n",
        "    self.vW = self.vW.tolist()\n",
        "\n",
        "    self.vB = self.beta*np.array(self.vB, dtype = object) + (1-self.beta)*(np.array(gB, dtype = object) **2 )\n",
        "    B = (1-self.lr*self.reg)*np.array(params['b'], dtype = object) - (self.lr/((self.vB + 1e-7)**0.5)) * np.array(gB, dtype = object)\n",
        "    self.vB = self.vB.tolist()\n",
        "\n",
        "    return W.tolist(), B.tolist()\n",
        "\n",
        "class Nesterov:   \n",
        "  def __init__(self, lr=0.01, gamma=0.9):\n",
        "    self.lr = lr\n",
        "    self.reg = None\n",
        "    self.gamma = gamma                                                             \n",
        "    self.Wmoments = None\n",
        "    self.Bmoments = None\n",
        "    self.activation_type = None\n",
        "    self.loss_type = None\n",
        "        \n",
        "  def update(self, w,b, gW, gB):\n",
        "    params = {'w':w, 'b':b}\n",
        "    if self.Wmoments == None:\n",
        "      self.Wmoments = [0] * len(params['w'])\n",
        "      for i in range(len(params['w'])):\n",
        "        self.Wmoments[i] = np.zeros_like(params['w'][i])\n",
        "\n",
        "    if self.Bmoments == None:\n",
        "      self.Bmoments = [0] * len(params['b'])\n",
        "      for i in range(len(params['b'])):\n",
        "        self.Bmoments[i] = np.zeros_like(params['b'][i])\n",
        "\n",
        "\n",
        "    W_look_ahead = np.array(params['w'], dtype = object) - self.gamma*np.array(self.Wmoments, dtype = object)\n",
        "    B_look_ahead = np.array(params['b'], dtype = object) - self.gamma*np.array(self.Bmoments, dtype = object)\n",
        "    ##\n",
        "    out, temp_h, temp_a = feed_forward(x,W_look_ahead.tolist(),B_look_ahead.tolist(), self.activation_type)\n",
        "    gW_look_ahead, gB_look_ahead, _, _ = back_prop(x, y,out,temp_a,temp_h, W_look_ahead.tolist(),B_look_ahead.tolist(), x.shape[1], self.loss_type, self.activation_type)\n",
        "\n",
        "    ###\n",
        "    self.Wmoments = self.gamma*np.array(self.Wmoments, dtype = object) + self.lr * np.array(gW_look_ahead, dtype = object)\n",
        "    self.Bmoments = self.gamma*np.array(self.Bmoments, dtype = object) + self.lr * np.array(gB_look_ahead, dtype = object)\n",
        "\n",
        "    W = (1-self.lr*self.reg)*np.array(params['w'], dtype = object) - self.Wmoments\n",
        "    self.Wmoments = self.Wmoments.tolist()\n",
        "\n",
        "    B = (1-self.lr*self.reg)*np.array(params['b'], dtype = object) - self.Bmoments\n",
        "    self.Bmoments = self.Bmoments.tolist()\n",
        "\n",
        "    return W.tolist(), B.tolist()\n"
      ],
      "metadata": {
        "id": "8K_GvR1b7mIc"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Train Loop\n",
        "def train(X_train, y_train,x_val, y_val, num_inputs_nodes, hidden_layers, num_output_nodes, init_type, epochs, batch_size, loss_type,activation_type, optimizer, learning_rate, reg_lamda):\n",
        "  W, B = param_inint(num_inputs_nodes,hidden_layers, num_output_nodes, init_type)\n",
        "  N = X_train.shape[0]\n",
        "  n_batches = int(np.floor(N/batch_size))\n",
        "  optimizer.lr = learning_rate\n",
        "  optimizer.reg = reg_lamda\n",
        "  ### For Nestrov and Nadam\n",
        "  try:   \n",
        "    optimizer.activation_type = activation_type\n",
        "    optimizer.loss_type = loss_type\n",
        "  except:\n",
        "    pass\n",
        "\n",
        "\n",
        "  for epoch in range(epochs):\n",
        "\n",
        "    train_loss = []\n",
        "    train_accuracy = []\n",
        "    val_loss = []\n",
        "    val_accuracy = []\n",
        "    l = 0\n",
        "    acc = 0\n",
        "    temp = 0\n",
        "    for batch in range(n_batches):\n",
        "      x = np.squeeze(X_train[batch*batch_size:batch_size+batch*batch_size]).T\n",
        "      y = one_hot(y_train[batch*batch_size:batch_size+batch*batch_size], 10)\n",
        "      y_hat, h, a = feed_forward(x, W,B, activation_type)\n",
        "      gw, gb, gh, ga = back_prop(x, y,y_hat,a,h, W,B, batch_size, loss_type, activation_type)\n",
        "      W,B = optimizer.update(W,B, gw,gb)\n",
        "      l += loss_compute(y_train[batch*batch_size:batch_size+batch*batch_size],y_hat, loss_type, W,reg_lamda)\n",
        "      acc += accuracy(y_hat, y_train[batch*batch_size:batch_size+batch*batch_size])\n",
        "\n",
        "    if N%batch_size != 0:\n",
        "        x = np.squeeze(X_train[-1*(N%batch_size):]).T\n",
        "        y = one_hot(y_train[-1*(N%batch_size):], 10)\n",
        "        y_hat, h, a = feed_forward(x, W,B, activation_type)\n",
        "        gw, gb, gh, ga = back_prop(x, y,y_hat,a,h, W,B, N%batch_size, loss_type, activation_type)\n",
        "        W,B = optimizer.update(W,B, gw,gb)\n",
        "        l += loss_compute(y_train[-1*(N%batch_size):],y_hat, loss_type, W,reg_lamda)\n",
        "        acc += accuracy(y_hat, y_train[-1*(N%batch_size):])\n",
        "        temp = 1\n",
        "\n",
        "    l = l/(n_batches + (N%batch_size))\n",
        "    acc = acc/(n_batches + temp)\n",
        "\n",
        "    train_loss.append(l)\n",
        "    train_accuracy.append(acc)\n",
        "    print(f\"Train Loss: {l}\")\n",
        "    print(f\"Train Accuracy: {acc}\")\n",
        "\n",
        "    #### Validation\n",
        "    if x_val.any():\n",
        "      y_val_hat, _,_ = feed_forward(np.squeeze(x_val).T, W,B, activation_type)\n",
        "      val_acc = accuracy(y_val_hat,y_val)\n",
        "      val_l = loss_compute(y_val, y_val_hat, loss_type,W = None, reg_lamda = reg_lamda)\n",
        "      val_accuracy.append(val_acc)\n",
        "      val_loss.append(val_l)\n",
        "      print(f\"Val Loss: {val_l}\")\n",
        "      print(f\"Val Accuracy: {val_acc}\")\n",
        "\n",
        "\n",
        "  return W,B, train_loss, train_accuracy, val_loss, val_accuracy\n"
      ],
      "metadata": {
        "id": "73NawZI67qKS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "6o1ucJuj8Gae"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "Fi5ANWCV8Iyx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Training and Prediction"
      ],
      "metadata": {
        "id": "dYgEc1_b8K0Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### Training\n",
        "optimizer = SGD()\n",
        "activation_type =  \"tanh\"\n",
        "W_new, B_new, _,_,_,_ = train(X_train, y_train, X_validation, y_validation, 784, [32,128,64], 10, \"xavier\", 5, 64, \"cross_entropy\", activation_type, optimizer,0.001, 0)\n",
        "\n",
        "#### Prediction\n",
        "#train\n",
        "Y, _, _ = feed_forward(np.squeeze(X_train).T, W_new, B_new, activation_type)\n",
        "Y_prediction_train = np.argmax(Y, axis = 0)\n",
        "\n",
        "#valid\n",
        "Y, _, _ = feed_forward(np.squeeze(X_validation).T, W_new, B_new, activation_type)\n",
        "Y_prediction_val = np.argmax(Y, axis = 0)\n",
        "\n",
        "#test\n",
        "Y, _, _= feed_forward(np.squeeze(X_test).T, W_new, B_new, activation_type)\n",
        "Y_prediction_test = np.argmax(Y, axis = 0)"
      ],
      "metadata": {
        "id": "kygLEaKn8GkB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "3S8H7Jtl8OqN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Accuracy"
      ],
      "metadata": {
        "id": "45ekCcyz8Pep"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Validation accuracy: {accuracy(Y_prediction_val, y_validation)}\")\n",
        "print(f\"Test accuracy: {accuracy(Y_prediction_test, y_test)}\")\n",
        "print(f\"Train accuracy: {accuracy(Y_prediction_train, y_train)}\")"
      ],
      "metadata": {
        "id": "k6upYp408QgE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}