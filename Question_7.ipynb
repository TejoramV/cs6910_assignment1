{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dtPrdYiS0Gp5"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "# CS6910 Assignment-1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KSt87eAa0Gp6"
      },
      "source": [
        "by\n",
        "- Akansh Maurya (CS22Z003)\n",
        "- Tejoram Vivekanandan (EE22Z001)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "P-vPuna71_pW"
      },
      "outputs": [],
      "source": [
        "from keras.datasets import fashion_mnist\n",
        "import numpy as np\n",
        "from  matplotlib import pyplot as plt\n",
        "import time\n",
        "import math\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tqdm.auto import tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KUZstREBMqW1"
      },
      "source": [
        "### Question 1: Loading and ploting the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "vk9LKi51h2ef",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d2074955-1fa2-4664-bfca-a265852d7755"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Dataset Shape:  (54000, 28, 28)\n",
            "Train Target Vector Shape:  (54000,)\n",
            "Test Dataset Shape: (10000, 28, 28)\n",
            "Test Target Vector Shape (10000,)\n",
            "Validation Dataset Shape: (6000, 28, 28)\n",
            "Validation Target Vector Shape (6000,)\n"
          ]
        }
      ],
      "source": [
        "dataset= fashion_mnist.load_data()\n",
        "(X_train_and_validation, y_train_and_validation), (X_test, y_test) = dataset\n",
        "X_train, X_validation, y_train, y_validation = train_test_split(X_train_and_validation, y_train_and_validation, test_size=0.1, random_state=42)\n",
        "X_train = (X_train/255.0).astype(np.float32)\n",
        "X_validation = (X_validation/255.0).astype(np.float32)\n",
        "X_test = (X_test/255.0).astype(np.float32)\n",
        "\n",
        "print(\"Train Dataset Shape: \", X_train.shape)\n",
        "print(\"Train Target Vector Shape: \", y_train.shape) \n",
        "print(\"Test Dataset Shape:\", X_test.shape)\n",
        "print(\"Test Target Vector Shape\", y_test.shape)\n",
        "print(\"Validation Dataset Shape:\", X_validation.shape)\n",
        "print(\"Validation Target Vector Shape\", y_validation.shape)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "lLZWiHVUkL05"
      },
      "outputs": [],
      "source": [
        "X_train = np.array(X_train.reshape(X_train.shape[0], 784,1))         \n",
        "X_test = np.array(X_test.reshape(X_test.shape[0], 784,1))\n",
        "X_validation = np.array(X_validation.reshape(X_validation.shape[0], 784,1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kgV1JgmlNcbx"
      },
      "source": [
        "**Implement a feedforward and backpropagation**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "5WdFI5xebOk-"
      },
      "outputs": [],
      "source": [
        "#Activation function\n",
        "def activation(activation_function):\n",
        "  if activation_function == 'sigmoid':\n",
        "    return sigmoid\n",
        "  if activation_function == 'tanh':\n",
        "    return tanh\n",
        "  if activation_function == 'ReLU':\n",
        "    return relu\n",
        "\n",
        "def sigmoid(x, derivative = False):\n",
        "  if derivative:\n",
        "    return sigmoid(x)*(1-sigmoid(x))\n",
        "  return 1/(1 + np.exp(-x))  \n",
        "\n",
        "def tanh(x, derivative = False):\n",
        "  if derivative:\n",
        "    return 1 - tanh(x)**2\n",
        "  return (np.exp(x)-np.exp(-x))/(np.exp(x)+np.exp(-x))\n",
        "\n",
        "def relu(x, derivative = False):\n",
        "  if derivative:\n",
        "    return (x>0)*1 \n",
        "  return x*(x>0)\n",
        "\n",
        "def softmax(x,derivative = False):\n",
        "  if derivative:\n",
        "    return softmax(x)*(1- softmax(x))\n",
        "  return np.exp(x)/np.sum(np.exp(x), axis = 0)\n",
        "\n",
        "def one_hot(y, num_output_nodes):\n",
        "  v = np.zeros((num_output_nodes, len(y)))\n",
        "  for i,j in enumerate(y):\n",
        "    v[j,i] = 1\n",
        "  return v\n",
        "\n",
        "### Weight Initialization\n",
        "def param_inint(num_inputs_nodes, hidden_layers, num_output_nodes, init_type):\n",
        "  W = []\n",
        "  B = []\n",
        "  if init_type == \"random\":\n",
        "    W.append(np.random.randn(hidden_layers[0],num_inputs_nodes)*0.1)\n",
        "    B.append(np.random.randn(hidden_layers[0], 1)*0.1)\n",
        "    for i in range(len(hidden_layers)-1):\n",
        "      W.append(np.random.randn(hidden_layers[i+1],hidden_layers[i])*0.1)\n",
        "      B.append(np.random.randn(hidden_layers[i+1], 1)*0.1)\n",
        "    W.append(np.random.randn(num_output_nodes, hidden_layers[-1])*0.1)\n",
        "    B.append(np.random.randn(num_output_nodes, 1)*0.1)\n",
        "    return W, B\n",
        "\n",
        "  if init_type == \"xavier\":\n",
        "    W.append(np.random.randn(hidden_layers[0],num_inputs_nodes)*np.sqrt(2/(hidden_layers[0] + num_inputs_nodes)))\n",
        "    B.append(np.random.randn(hidden_layers[0], 1)*0.1)\n",
        "    for i in range(len(hidden_layers)-1):\n",
        "      W.append(np.random.randn(hidden_layers[i+1],hidden_layers[i])*np.sqrt(2/(hidden_layers[i+1] + hidden_layers[i])))\n",
        "      B.append(np.random.randn(hidden_layers[i+1], 1)*0.1)\n",
        "    W.append(np.random.randn(num_output_nodes, hidden_layers[-1])*np.sqrt(2/(num_output_nodes + hidden_layers[-1])))\n",
        "    B.append(np.random.randn(num_output_nodes, 1)*0.1)\n",
        "    return W, B\n",
        "\n",
        "\n",
        "def feed_forward(x, W, B, activation_type):\n",
        "  h = []\n",
        "  a = []\n",
        "  sigma = activation(activation_type)  #activation\n",
        "  h.append(x)   #h0 = x\n",
        "  a.append(np.dot(W[0], h[0]) + B[0])\n",
        "  for i in range(len(W)-1):\n",
        "    h.append(sigma(a[-1]))\n",
        "    a.append(np.dot(W[i+1], h[-1]) + B[i+1])\n",
        "  y_hat = softmax(a[-1])\n",
        "\n",
        "  return y_hat, h, a\n",
        "\n",
        "\n",
        "\n",
        "def loss_compute(y,y_hat, loss_type, W, reg_lamda):\n",
        "  if loss_type == \"squared_error\":\n",
        "    error = np.sum((one_hot(y, 10)-y_hat)**2)/(2*one_hot(y, 10).shape[1])\n",
        "  if loss_type == \"cross_entropy\":\n",
        "    error = -1*np.sum(np.multiply(one_hot(y, 10),np.log(y_hat)))/one_hot(y, 10).shape[1]         # hardcoded classes = 10\n",
        "\n",
        "  if W:\n",
        "    r = 0\n",
        "    for i in range(len(W)):\n",
        "      r += np.sum((np.array(W, dtype = object) **2)[i])\n",
        "    error = error + reg_lamda * r\n",
        "\n",
        "  return error\n",
        "\n",
        "\n",
        "def accuracy(y_hat, y_true):\n",
        "  return np.mean(np.argmax(y_hat, axis = 0) ==y_true )*100\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "buMsoNw2IIsI"
      },
      "source": [
        "##### Back Propogation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "JJfnib-7IIsJ"
      },
      "outputs": [],
      "source": [
        "def back_prop(x, y, y_hat, a, h , W, B, batch_size, loss_type, activation_type):\n",
        "  gh = [0]*len(h)\n",
        "  ga = [0]*len(a)\n",
        "  gw = [0]*len(W)\n",
        "  gb = [0]*len(B)\n",
        "\n",
        "  sigma = activation(activation_type) \n",
        "\n",
        "  if loss_type == \"cross_entropy\":\n",
        "    gh[-1] = -1*(y/y_hat)\n",
        "    ga[-1] = -1*(y-y_hat)\n",
        "  if loss_type == \"squared_error\":   ##### edit this\n",
        "    gh[-1] = y_hat - y\n",
        "    ga[-1] = (y_hat - y)*softmax(a[-1])*(1-softmax(a[-1]))\n",
        "\n",
        "  for i in range(len(W)-1, -1, -1):\n",
        "    gw[i] = np.dot(ga[i], h[i].T)\n",
        "    gb[i] = np.dot(ga[i], np.ones((batch_size,1)))\n",
        "    if i > 0:\n",
        "      gh[i-1] = np.dot(W[i].T, ga[i])\n",
        "      ga[i-1]  = np.multiply(gh[i-1],sigma(a[i-1], derivative = True))\n",
        "\n",
        "  return gw, gb, gh, ga"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5LcMwwu4JS-Y"
      },
      "source": [
        "**Optimizing functions**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "recqY3V8L1Uk"
      },
      "outputs": [],
      "source": [
        "class SGD:\n",
        "  ''' Stochastic Gradient Descent '''\n",
        "  def __init__(self, lr = 0.001, reg = 0):\n",
        "    self.lr = lr\n",
        "    self.reg = reg\n",
        "  \n",
        "  def update(self, w,b, gW, gB):\n",
        "    W = np.array(w, dtype = object)\n",
        "    B = np.array(b, dtype = object)\n",
        "\n",
        "    W = (1-self.lr*self.reg)*W - self.lr * np.array(gW, dtype = object)\n",
        "    B = (1-self.lr*self.reg)*B - self.lr * np.array(gB, dtype = object)\n",
        "\n",
        "    return W.tolist(),B.tolist()\n",
        "\n",
        "\n",
        "class Momentum:\n",
        "\n",
        "  def __init__(self, lr = 0.001, gamma = 0.9, reg = 0):\n",
        "    self.lr = lr\n",
        "    self.gamma = gamma\n",
        "    self.Wmoments = None\n",
        "    self.Bmoments = None\n",
        "    self.reg = reg\n",
        "\n",
        "\n",
        "  def update(self, w,b, gW, gB):\n",
        "    params = {'w':w, 'b':b}\n",
        "\n",
        "    if self.Wmoments == None:\n",
        "      self.Wmoments = [0] * len(params['w'])\n",
        "      for i in range(len(params['w'])):\n",
        "        self.Wmoments[i] = np.zeros_like(params['w'][i])\n",
        "\n",
        "    if self.Bmoments == None:\n",
        "      self.Bmoments = [0] * len(params['b'])\n",
        "      for i in range(len(params['b'])):\n",
        "        self.Bmoments[i] = np.zeros_like(params['b'][i])\n",
        "    \n",
        "    self.Wmoments = self.gamma * np.array(self.Wmoments, dtype = object) + self.lr * np.array(gW, dtype = object)\n",
        "    W = (1-self.lr*self.reg)*np.array(params['w'], dtype = object) - self.Wmoments\n",
        "    self.Wmoments = self.Wmoments.tolist()\n",
        "\n",
        "    self.Bmoments = self.gamma * np.array(self.Bmoments, dtype = object) + self.lr * np.array(gB, dtype = object)\n",
        "    B = (1-self.lr*self.reg)*np.array(params['b'], dtype = object) - self.Bmoments\n",
        "    self.Bmoments = self.Bmoments.tolist()\n",
        "    \n",
        "    return W.tolist(), B.tolist()\n",
        "\n",
        "\n",
        "class RMSprop:\n",
        "  def __init__(self, lr=0.01, beta = 0.99):\n",
        "    \n",
        "    self.lr = lr\n",
        "    self.vW = None\n",
        "    self.vB = None\n",
        "    self.beta = beta\n",
        "\n",
        "  def update(self, w,b, gW, gB):\n",
        "    params = {'w':w, 'b':b}\n",
        "    if self.vW == None:\n",
        "      self.vW = [0] * len(params['w'])\n",
        "      for i in range(len(params['w'])):\n",
        "        self.vW[i] = np.zeros_like(params['w'][i])\n",
        "\n",
        "    if self.vB == None:\n",
        "      self.vB = [0] * len(params['b'])\n",
        "      for i in range(len(params['b'])):\n",
        "        self.vB[i] = np.zeros_like(params['b'][i])\n",
        "\n",
        "    self.vW = self.beta*np.array(self.vW, dtype = object) + (1-self.beta)*(np.array(gW, dtype = object) **2) \n",
        "    W = (1-self.lr*self.reg)*np.array(params['w'], dtype = object) - (self.lr/((self.vW + 1e-7)**0.5)) * np.array(gW, dtype = object)\n",
        "    self.vW = self.vW.tolist()\n",
        "\n",
        "    self.vB = self.beta*np.array(self.vB, dtype = object) + (1-self.beta)*(np.array(gB, dtype = object) **2 )\n",
        "    B = (1-self.lr*self.reg)*np.array(params['b'], dtype = object) - (self.lr/((self.vB + 1e-7)**0.5)) * np.array(gB, dtype = object)\n",
        "    self.vB = self.vB.tolist()\n",
        "\n",
        "    return W.tolist(), B.tolist()\n",
        "\n",
        "class Nesterov:   \n",
        "  def __init__(self, lr=0.01, gamma=0.9):\n",
        "    self.lr = lr\n",
        "    self.reg = None\n",
        "    self.gamma = gamma                                                             \n",
        "    self.Wmoments = None\n",
        "    self.Bmoments = None\n",
        "    self.activation_type = None\n",
        "    self.loss_type = None\n",
        "        \n",
        "  def update(self, w,b, gW, gB):\n",
        "    params = {'w':w, 'b':b}\n",
        "    if self.Wmoments == None:\n",
        "      self.Wmoments = [0] * len(params['w'])\n",
        "      for i in range(len(params['w'])):\n",
        "        self.Wmoments[i] = np.zeros_like(params['w'][i])\n",
        "\n",
        "    if self.Bmoments == None:\n",
        "      self.Bmoments = [0] * len(params['b'])\n",
        "      for i in range(len(params['b'])):\n",
        "        self.Bmoments[i] = np.zeros_like(params['b'][i])\n",
        "\n",
        "\n",
        "    W_look_ahead = np.array(params['w'], dtype = object) - self.gamma*np.array(self.Wmoments, dtype = object)\n",
        "    B_look_ahead = np.array(params['b'], dtype = object) - self.gamma*np.array(self.Bmoments, dtype = object)\n",
        "    ##\n",
        "    out, temp_h, temp_a = feed_forward(x,W_look_ahead.tolist(),B_look_ahead.tolist(), self.activation_type)\n",
        "    gW_look_ahead, gB_look_ahead, _, _ = back_prop(x, y,out,temp_a,temp_h, W_look_ahead.tolist(),B_look_ahead.tolist(), x.shape[1], self.loss_type, self.activation_type)\n",
        "\n",
        "    ###\n",
        "    self.Wmoments = self.gamma*np.array(self.Wmoments, dtype = object) + self.lr * np.array(gW_look_ahead, dtype = object)\n",
        "    self.Bmoments = self.gamma*np.array(self.Bmoments, dtype = object) + self.lr * np.array(gB_look_ahead, dtype = object)\n",
        "\n",
        "    W = (1-self.lr*self.reg)*np.array(params['w'], dtype = object) - self.Wmoments\n",
        "    self.Wmoments = self.Wmoments.tolist()\n",
        "\n",
        "    B = (1-self.lr*self.reg)*np.array(params['b'], dtype = object) - self.Bmoments\n",
        "    self.Bmoments = self.Bmoments.tolist()\n",
        "\n",
        "    return W.tolist(), B.tolist()\n",
        "\n",
        "class Adam:\n",
        "  def __init__(self, lr=0.001, beta1=0.9, beta2=0.999, reg = 0):\n",
        "    self.lr = lr\n",
        "    self.beta1 = beta1\n",
        "    self.beta2 = beta2\n",
        "    self.t = 0\n",
        "    self.mW = None\n",
        "    self.vW = None\n",
        "    self.mB = None\n",
        "    self.vB = None\n",
        "    self.reg = None\n",
        "        \n",
        "  def update(self, w,b, gW, gB):\n",
        "    params = {'w':w, 'b':b}\n",
        "\n",
        "    if self.mW is None:\n",
        "      self.mW, self.vW = [0] * len(params['w']), [0] * len(params['w'])\n",
        "      for i in range(len(params['w'])):\n",
        "        self.mW[i] = np.zeros_like(params['w'][i])\n",
        "        self.vW[i] = np.zeros_like(params['w'][i])\n",
        "\n",
        "    if self.mB is None:\n",
        "      self.mB, self.vB = [0] * len(params['b']), [0] * len(params['b'])\n",
        "      for i in range(len(params['b'])):\n",
        "        self.mB[i] = np.zeros_like(params['b'][i])\n",
        "        self.vB[i] = np.zeros_like(params['b'][i])\n",
        "    \n",
        "\n",
        "    self.t += 1\n",
        "    self.mW = (self.beta1 * np.array(self.mW, dtype = object)) + (1-self.beta1)*(np.array(gW, dtype = object))\n",
        "    self.vW = (self.beta2 * np.array(self.vW, dtype = object)) + (1-self.beta2)*((np.array(gW, dtype = object)**2))\n",
        "\n",
        "    self.mB = (self.beta1 * np.array(self.mB, dtype = object)) + (1-self.beta1)*(np.array(gB, dtype = object))\n",
        "    self.vB = (self.beta2 * np.array(self.vB, dtype = object)) + (1-self.beta2)*((np.array(gB, dtype = object)**2))\n",
        "\n",
        "    # Bias Correction\n",
        "    self.mW = (self.mW)*(1.0/(1-(self.beta1**self.t)))\n",
        "    self.vW = (self.vW)*(1.0/(1-(self.beta2**self.t)))\n",
        "    self.mB = (self.mB)*(1.0/(1-(self.beta1**self.t)))\n",
        "    self.vB = (self.vB)*(1.0/(1-(self.beta2**self.t)))\n",
        "\n",
        "    W = (1-self.lr*self.reg)*np.array(params['w'], dtype = object) - (self.lr/((self.vW + 1e-7)**0.5)) * self.mW\n",
        "    self.vW = self.vW.tolist()\n",
        "    self.mW = self.mW.tolist()\n",
        "\n",
        "    B = (1-self.lr*self.reg)*np.array(params['b'], dtype = object) - (self.lr/((self.vB + 1e-7)**0.5)) * self.mB\n",
        "    self.vB = self.vB.tolist()\n",
        "    self.mB = self.mB.tolist()\n",
        "\n",
        "    return W.tolist(), B.tolist()    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5MLzYFPHIIsL"
      },
      "source": [
        "##### Training Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "es-CTkdsIIsM"
      },
      "outputs": [],
      "source": [
        "def train(X_train, y_train,x_val, y_val, num_inputs_nodes, hidden_layers, num_output_nodes, init_type, epochs, batch_size, loss_type,activation_type, optimizer_name, learning_rate, reg_lamda):\n",
        "  if optimizer_name=='sgd':\n",
        "    optimizer = SGD()\n",
        "  elif optimizer_name=='momentum':\n",
        "    optimizer = Momentum()\n",
        "  elif optimizer_name=='rmsprop':\n",
        "    optimizer = RMSprop()\n",
        "  elif optimizer_name=='nesterov':\n",
        "    optimizer = Nesterov()\n",
        "  elif optimizer_name=='adam':\n",
        "    optimizer = Adam()    \n",
        "  \n",
        "\n",
        "  try:   \n",
        "    optimizer.activation_type = activation_type\n",
        "    optimizer.loss_type = loss_type\n",
        "  except:\n",
        "    pass\n",
        "\n",
        "  W, B = param_inint(num_inputs_nodes,hidden_layers, num_output_nodes, init_type)\n",
        "  N = X_train.shape[0]\n",
        "  n_batches = int(np.floor(N/batch_size))\n",
        "  optimizer.lr = learning_rate\n",
        "  optimizer.reg = reg_lamda\n",
        "\n",
        "  for epoch in range(epochs):\n",
        "\n",
        "    train_loss = []\n",
        "    train_accuracy = []\n",
        "    val_loss = []\n",
        "    val_accuracy = []\n",
        "    l = 0\n",
        "    acc = 0\n",
        "    temp = 0\n",
        "    for batch in range(n_batches):\n",
        "      x = np.squeeze(X_train[batch*batch_size:batch_size+batch*batch_size]).T\n",
        "      y = one_hot(y_train[batch*batch_size:batch_size+batch*batch_size], 10)\n",
        "      y_hat, h, a = feed_forward(x, W,B, activation_type)\n",
        "      gw, gb, gh, ga = back_prop(x, y,y_hat,a,h, W,B, batch_size, loss_type, activation_type)\n",
        "      W,B = optimizer.update(W,B, gw,gb)\n",
        "      l += loss_compute(y_train[batch*batch_size:batch_size+batch*batch_size],y_hat, loss_type, W,reg_lamda)\n",
        "      acc += accuracy(y_hat, y_train[batch*batch_size:batch_size+batch*batch_size])\n",
        "\n",
        "    if N%batch_size != 0:\n",
        "        x = np.squeeze(X_train[-1*(N%batch_size):]).T\n",
        "        y = one_hot(y_train[-1*(N%batch_size):], 10)\n",
        "        y_hat, h, a = feed_forward(x, W,B, activation_type)\n",
        "        gw, gb, gh, ga = back_prop(x, y,y_hat,a,h, W,B, N%batch_size, loss_type, activation_type)\n",
        "        W,B = optimizer.update(W,B, gw,gb)\n",
        "        l += loss_compute(y_train[-1*(N%batch_size):],y_hat, loss_type, W,reg_lamda)\n",
        "        acc += accuracy(y_hat, y_train[-1*(N%batch_size):])\n",
        "        temp = 1\n",
        "\n",
        "    l = l/(n_batches + (N%batch_size))\n",
        "    acc = acc/(n_batches + temp)\n",
        "\n",
        "    train_loss.append(l)\n",
        "    train_accuracy.append(acc)\n",
        "    #print(f\"Epoch:{epoch+1}\")\n",
        "    #print(f\"Train Loss: {l}\")\n",
        "    #print(f\"Train Accuracy: {acc}\")\n",
        "\n",
        "    #### Validation\n",
        "    if x_val.any():\n",
        "      y_val_hat, _,_ = feed_forward(np.squeeze(x_val).T, W,B, activation_type)\n",
        "      val_acc = accuracy(y_val_hat,y_val)\n",
        "      val_l = loss_compute(y_val, y_val_hat, loss_type,W = None, reg_lamda = reg_lamda)\n",
        "      val_accuracy.append(val_acc)\n",
        "      val_loss.append(val_l)\n",
        "      #print(f\"Val Loss: {val_l}\")\n",
        "      #print(f\"Val Accuracy: {val_acc}\")\n",
        "\n",
        "    wandb.log({\"epoch\":epoch,\"Train_loss\":l,\"Train_acc\":acc,\"val_loss\":val_l,\"val_Accuracy\":val_acc})\n",
        "  return W,B, train_loss, train_accuracy, val_loss, val_accuracy\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wandb\n",
        "!wandb login\n",
        "import wandb\n",
        "wandb.init(project=\"CS6910-Assignment-1\", entity=\"tejoram\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 635
        },
        "id": "Y3NFYTPLX-Pw",
        "outputId": "2b9add44-1892-4e74-a656-16738b719365"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: wandb in /usr/local/lib/python3.7/dist-packages (0.12.11)\n",
            "Requirement already satisfied: Click!=8.0.0,>=7.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (7.1.2)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.23.0)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (5.4.8)\n",
            "Requirement already satisfied: six>=1.13.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (1.15.0)\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.7/dist-packages (from wandb) (1.2.2)\n",
            "Requirement already satisfied: yaspin>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.1.0)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from wandb) (3.13)\n",
            "Requirement already satisfied: protobuf>=3.12.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (3.17.3)\n",
            "Requirement already satisfied: sentry-sdk>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (1.5.6)\n",
            "Requirement already satisfied: pathtools in /usr/local/lib/python3.7/dist-packages (from wandb) (0.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.8.2)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (0.4.0)\n",
            "Requirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.3)\n",
            "Requirement already satisfied: shortuuid>=0.5.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (1.0.8)\n",
            "Requirement already satisfied: GitPython>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (3.1.27)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.7/dist-packages (from GitPython>=1.0.0->wandb) (4.0.9)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from GitPython>=1.0.0->wandb) (3.10.0.2)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.7/dist-packages (from gitdb<5,>=4.0.1->GitPython>=1.0.0->wandb) (5.0.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2021.10.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (1.24.3)\n",
            "Requirement already satisfied: termcolor<2.0.0,>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from yaspin>=1.0.0->wandb) (1.1.0)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mtejoram\u001b[0m (use `wandb login --relogin` to force relogin)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg entity when running a sweep.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.12.11"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20220302_160525-sbg4d8yq</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "Syncing run <strong><a href=\"https://wandb.ai/tejoram/CS6910-Assignment-1/runs/sbg4d8yq\" target=\"_blank\">e_5_hl_[64, 32, 16]_lr_0.001_wd_0.0005_o_momentum_bs_16_winit_random_ac_sigmoid_los_cross_entropy_r_0</a></strong> to <a href=\"https://wandb.ai/tejoram/CS6910-Assignment-1\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>Sweep page:  <a href=\"https://wandb.ai/tejoram/CS6910-Assignment-1/sweeps/yttmvpj4\" target=\"_blank\">https://wandb.ai/tejoram/CS6910-Assignment-1/sweeps/yttmvpj4</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<wandb.sdk.wandb_run.Run at 0x7f3438d21ad0>"
            ],
            "text/html": [
              "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src=\"https://wandb.ai/tejoram/CS6910-Assignment-1/runs/sbg4d8yq?jupyter=true\" style=\"border:none;width:100%;height:420px;display:none;\"></iframe>"
            ]
          },
          "metadata": {},
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "W_new, B_new, _,_,_,_ = train(X_train, y_train, X_validation, y_validation, 784, [128,64,32], 10, \"xavier\", 10, 64, \"cross_entropy\", \"ReLU\", 'rmsprop',0.001, 0.5)\n",
        "Y, _, _ = feed_forward(np.squeeze(X_validation).T, W_new, B_new, \"ReLU\")\n",
        "\n",
        "Y_prediction = np.argmax(Y, axis = 0)\n",
        "\n",
        " ##change login\n",
        "print(y_train.shape) ### I have implemeted for batch size of 50 change this index (and forecoming) for whole date ..also Y sd be from whole data\n",
        "\n",
        "labels_dict_names =  [\"T-shirt/top\",\"Trouser\",\"Pullover\",\"Dress\",\"Coat\",\"Sandal\",\"Shirt\",\"Sneaker\",\"Bag\",\"Ankle boot\"]\n",
        "\n",
        "\n",
        "# Y_prediction=np.empty(np.shape(y_train)) ##index change\n",
        "# print(len(Y_prediction))\n",
        "# for i in range(len(Y_prediction)):\n",
        "#   Y_prediction[i]=np.argmax(Y[:,i])\n",
        "\n",
        "wandb.log({\"Confusion matrix\": wandb.plot.confusion_matrix(probs=None,y_true=y_validation,preds=Y_prediction,class_names=labels_dict_names)})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RpviJDsrUaQ1",
        "outputId": "efaccef9-b783-4883-d755-949c3cb8cc13"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(54000,)\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Assignment_1.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}